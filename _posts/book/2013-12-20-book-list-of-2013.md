---
layout: post
title: 2013阅读书单
category: 读书
keywords: 阅读,书单,2013
---
## 绪论
摘录书中几句有意思的话
1. “没有免费的午餐定理”（NFL）说明脱离具体的问题，空谈什么算法好，毫无意义。

2. 以往机器学习在应用中要取得很好的性能对使用者要求较高，而深度学习只要下功夫调参，把参数调节好性能往往就好。降低了门槛，为机器学习走向工程带来便利。
---
## 模型评估与选择
在研究对比不同算法的泛化性能时使用测试集，在模型选择和调参时，使用验证集。
### 1. 经验误差与过拟合
* 错误率（error rate）：预测错误样本占总数比例
* 精度（accuracy）：1 - 错误率
* 误差(error)：预测输出与实际输出之间的差异。学习器在训练集上的误差为训练误差或经验误差；在外部数据上的误差叫做泛化误差。
* 过拟合(overfitting)：学习器把训练样本学的太好，以至于将一些非全局的特性都学到了这就导致了泛化能力的下降。主要是由于学习器学习能力太强。过拟合是机器学习所面临的关键障碍，无法彻底避免，只能缓解。
* 模型选择(model selection)：现实任务往往多种学习器选择，或同一学习器，不同参数配置也会产生不同的模型。这就出现了模型选择问题。
*  **怎么选择** ？理想的解决方案是对候选模型的泛化误差来评估，然后选泛化误差最小的。**然而**，无法直接获得泛化误差，而训练误差由于过拟合的存在也不合适。所以说怎么选择是个问题，我们在下节讨论。

### 2. 怎么来评估模型：模型的评估方法
那现在就是说了怎么来评估模型好坏从而选择模型呢？

ANS:通常，通过实验测试来对学习器的泛化误差进行评估从而进行模型选择。为此需要一个验证集，来测试学习器对新样本的判别能力，以验证集上的验证误差作为泛化误差的近似。

那怎么产生各种集呢？

ANS:不着急，看下面讲解。

2.1 训练集合测试集的产生
* 留出法(hand-out)：最简单，直接将数据集划分为两个互斥的集合。
  > 需要注意训练测试的划分要保证数据分布的一致性。

  > 即便给定了训练集和测试集的样本比例，单次划分的估计结果往往不够稳定，要多次划分求均值作为留出法的评估结果。
  
  > 测试集小时，评估结果方差较大；训练集小时，评估结果偏差较大。一般是大约2/3~4/5作为训练，其余做测试。
  
* 交叉验证法(Cross validation)：现将数据划分为K个大小相似的互斥的子集，然后以k-1个子集的并集作为训练，其余作为测试。这样可以进行K次训练测试，得到K组结果，最终返回K个结果的均值。

  >留一法一般被认为比较准确，然而数据集较大时，计算开销难以忍受。
* 自助法(bootstrapping)

   前两种方法由于保留部分样本作为测试，因此实际评估的模型使用的训练集要小。会导致因训练样本规模不同导致的估计偏差。**留一法**还好。为解决这个问题提出了自助法。
   
   什么是自助法？
   
   ANS : 给定包含m个样本的数据D,对他进行数据采样产生数据集D'。具体方法是，每次从D中挑一个样本放入D',这样有放回的抽m次，得到了包含m个样本的D'。通过计算，样本在m次中均不被抽到概率为0.368。于是我们将D'作为训练集，始终未被抽到的那些作为测试集。这样实际评估的模型与期望评估的模型都有m个训练样本，而且仍有约占数据总量1/3的数据做测试。这也称为**包外估计**(out-of-bag estimate)。
   
   什么时候使用该方法？
   
   ANS：数据集较小，难以有效划分训练测试时很有效。另外该方法可以从数据中产生多个不同训练集，对集成学习也有好处。
   
   缺点呢？
   
   ANS：产生的数据集改变了原有数据集的分布，将引入估计偏差。因此，数据足够时留出法和交叉验证使用的多一些。
   
2.2 假设选定了模型，还要做什么：调参(也是一种模型选择)

**在模型选择完成后，学习算法和参数配置也都选定，此时应该用数据集D重新训练模型，这才是我们的最终模型。** 


### 3. 用什么值来评估：模型的性能度量
### 4. 得到评估结果了怎么比较：比较检验
### 5. 偏差与方差

---
[夜间模式](第一章.html)
