---
layout: post
title: 机器学习绪论与模型评价
category: 机器学习
keywords: ML,2017
---
## 绪论
摘录书中几句有意思的话
1. “没有免费的午餐定理”(NFL)说明脱离具体的问题，空谈什么算法好，毫无意义。

2. 以往机器学习在应用中要取得很好的性能对使用者要求较高，而深度学习只要下功夫调参，把参数调节好性能往往就好。降低了门槛，为机器学习走向工程带来便利。

---
## 模型评估与选择
在研究对比不同算法的泛化性能时使用测试集，在模型选择和调参时，使用验证集。
### 1. 经验误差与过拟合
* 错误率(error rate)：预测错误样本占总数比例
* 精度(accuracy)：1 - 错误率
* 误差(error)：预测输出与实际输出之间的差异。学习器在训练集上的误差为训练误差或经验误差；在外部数据上的误差叫做泛化误差。
* 过拟合(overfitting)：学习器把训练样本学的太好，以至于将一些非全局的特性都学到了这就导致了泛化能力的下降。主要是由于学习器学习能力太强。过拟合是机器学习所面临的关键障碍，无法彻底避免，只能缓解。
* 模型选择(model selection)：现实任务往往多种学习器选择，或同一学习器，不同参数配置也会产生不同的模型。这就出现了模型选择问题。
*  **怎么选择** ？理想的解决方案是对候选模型的泛化误差来评估，然后选泛化误差最小的。**然而**，无法直接获得泛化误差，而训练误差由于过拟合的存在也不合适。所以说怎么选择是个问题，我们在下节讨论。

### 2. 怎么来评估模型：模型的评估方法
那现在就是说了怎么来评估模型好坏从而选择模型呢？

ANS:通常，通过实验测试来对学习器的泛化误差进行评估从而进行模型选择。为此需要一个验证集，来测试学习器对新样本的判别能力，以验证集上的验证误差作为泛化误差的近似。

那怎么产生各种集呢？

ANS:不着急，看下面讲解。

#### 2.1 训练集和测试集的产生
* 留出法(hand-out)：最简单，直接将数据集划分为两个互斥的集合。
  + 需要注意训练测试的划分要保证数据分布的一致性。

  + 即便给定了训练集和测试集的样本比例，单次划分的估计结果往往不够稳定，要多次划分求均值作为留出法的评估结果。

  + 测试集小时，评估结果方差较大；训练集小时，评估结果偏差较大。一般是大约2/3~4/5作为训练，其余做测试。

* 交叉验证法(Cross validation)：现将数据划分为K个大小相似的互斥的子集，然后以k-1个子集的并集作为训练，其余作为测试。这样可以进行K次训练测试，得到K组结果，最终返回K个结果的均值。

  * 留一法一般被认为比较准确，然而数据集较大时，计算开销难以忍受。
* 自助法(bootstrapping)

   前两种方法由于保留部分样本作为测试，因此实际评估的模型使用的训练集要小。会导致因训练样本规模不同导致的估计偏差。**留一法**还好。为解决这个问题提出了自助法。

   什么是自助法？

   ANS : 给定包含m个样本的数据D,对他进行数据采样产生数据集D'。具体方法是，每次从D中挑一个样本放入D',这样有放回的抽m次，得到了包含m个样本的D'。通过计算，样本在m次中均不被抽到概率为0.368。于是我们将D'作为训练集，始终未被抽到的那些作为测试集。这样实际评估的模型与期望评估的模型都有m个训练样本，而且仍有约占数据总量1/3的数据做测试。这也称为**包外估计**(out-of-bag estimate)。

   什么时候使用该方法？

   ANS：数据集较小，难以有效划分训练测试时很有效。另外该方法可以从数据中产生多个不同训练集，对集成学习也有好处。

   缺点呢？

   ANS：产生的数据集改变了原有数据集的分布，将引入估计偏差。因此，数据足够时留出法和交叉验证使用的多一些。

#### 2.2 选定了模型，还要做什么：调参  
调参也是一种模型选择。

**在模型选择完成后，学习算法和参数配置也都选定，此时应该用数据集D重新训练模型，这才是我们的最终模型。**


### 3. 用什么值来评估：模型的性能度量  
#### 3.1 回归任务

回归任务中，最常使用的性能就是 **均方误差**(mean squared error)
\[E(f;D) = \frac {1}{m}\sum_{i=1}^m(f(x_i)-y_i)^2\]
另外一个常用的就是$R^2$
\[R^2 = 1-\frac{SSE}{SST}\]
\[SSE = \sum_{i=1}^m(f(x_i)-y_i)^2\]
\[SST = \sum_{i=1}^m(y_i-\overline y)^2\]
其中SSE(Sum of Squares for Error)。

#### 3.2 分类问题
* 错误率和精度

\[E(f;D) = \frac{1}{m}\sum_{i=1}^mI(f(x_i)\neq y_i)\]
\[acc(f;D) = 1-E(f;D)\]

* 查准率、查全率与F1  
查准率(Precision)：我挑出来的瓜里面有多少是好的(所谓我挑出来的，就是我认为它是好的，也就是所有的预测Positive)  
 查全率(Recall)：所有好的瓜里面我挑出来多少(所谓所有的好瓜即所有的实际Positive)  
 \[P =\frac{TP}{TP+FP}\]
 \[P =\frac{TP}{TP+FN}\]
* F1值  
  全了不一定准，准了又不一定全，我们需要一个平衡点这就  是**F1值**(基于查准率和查全率的调和评价)
  \[F1 = \frac{2\times P\times R}{P+R}\]  
  在有些问题中，由于查全率和查准率的重要 程度不同，因此引入了F1的一般形式 $$F_{\beta}$$，可以定义对查准率和查全率的不同偏好。  
  \[F_{\beta} = \frac{(1+\beta^2)\times P\times R}{(\beta^2\times P)+R}\]
  其中 $\beta$ 度量了查全率/查准率的相对重要性。$\beta>1$,说明查全率影响更大，反之，查准率影响更大。  
  另外，关于对多次训练/测试计算“宏”和“微”的问题，先算出最终结果再计算平均叫“宏”，先计算中间值平均(如TP等)在计算最终结果叫“微”。
* ROC和AUC  
很多分类器其实产生的是一个实值或概率预测，然后将预测值与设定的分类阈值比较，大于阈值非为正类，小于分为反类。
一般情况下，若重视**查准率**，那就把阈值设大一点，反之设小一点。  
ROC全程“受试者工作特征”(Receiver Operating Characteristic)。其纵轴为**真正例率**(TPR)，横轴为**假正例率**(FPR)。
  \[ TPR = \frac{TP}{TP+FN}\]
  \[ FPR = \frac{FP}{TN+FP}\]
  其过程如下：
  * 给定 $m^+$ 个正例和 $m^-$ 个反例
  * 按照预测结果对样例降序排列
  * 把分裂阈值设为最大(将所有结果均预测为反例)，分别求出TPR和FPR。
  * 然后将阈值依次设为每个样例的预测值，依次求出TPR和FPR。

  在求坐标过程中，若前一个点的坐标为(x,y) ，若该点为真正例，则该点坐标为 $(x,y+\frac{1}{m^+})$；若该点为假正例，则该点坐标为 $(x+\frac{1}{m^-},y)$。  

  **AUC**是ROC曲线下面积，其考虑的是样本预测的排序质量，等于1减去排序损失。可以通过如下计算公式估算：
  \[AUC = \frac1{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)\cdot(y_i+y_{i+1})\]
* 代价敏感率和代价曲线  
在现实生活中，不同的错误常导致的后果不同。如在医疗诊断中将健康人诊断为病人**和**将病人诊断为健康人，显然后者损失更大。为了均衡不同错误类型带来的不同损失，可以为错误赋予“非均等代价”。  
可以通过设定**代价矩阵来实现：**  
![代价矩阵](http://otgv2i3k9.bkt.clouddn.com/markdown-img-paste-20170723142843849.png)  

  若将第0类判别为第1类损失更大，则 $cost_{01}>cost_{10}$，损失程度相差越大，两者的值差别越大。因此在非均等的代价下，我们希望的就不仅仅是简单的最小化错误次数，而是希望最小化总体代价。
  为此提出了代价敏感的错误率：  
  \[E(f;D;cost) = \frac1 m(\sum_{x_i\in D^+}I(f(x_i)\neq y_i)\times cost_{01}+\]  
\[\sum_{x_i\in D^-}I(f(x_i)\neq y_i)\times cost_{10})\]
### 4. 得到评估结果了怎么比较：比较检验  
![](http://latex.codecogs.com/gif.latex?\\frac{1}{1+sin(x)})
### 5. 偏差与方差
```python
print(i)
```
---
[夜间模式](http://zxst.github.io/第一章.html)
